import os
from dotenv import load_dotenv

# 1. åŠ è½½ .env é‡Œçš„ API Key
load_dotenv()

# æ£€æŸ¥ key æ˜¯å¦åŠ è½½æˆåŠŸ
if not os.getenv("OPENAI_API_KEY"):
    print("âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ° API Keyï¼Œè¯·æ£€æŸ¥ .env æ–‡ä»¶")
    exit()

from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain.schema import Document
from langchain.prompts import ChatPromptTemplate

# === A. å‡†å¤‡æ•°æ® (ä»¥åè¿™é‡Œæ¢æˆè¯»å– PDF çš„ä»£ç ) ===
print("ğŸ”„ æ­£åœ¨åˆå§‹åŒ–æ•°æ®...")
steel_data = [
    {
        "name": "316Lä¸é”ˆé’¢",
        "content": "ç‰Œå·ï¼š316Lã€‚åŒ–å­¦æˆåˆ†ï¼šé“¬16-18%ï¼Œé•10-14%ï¼Œé’¼2-3%ã€‚ç‰¹æ€§ï¼šå«é’¼ï¼Œè€æµ·æ´‹å’ŒåŒ–å·¥è…èš€ï¼ŒæŠ—æ°¯ç¦»å­è…èš€èƒ½åŠ›ä¼˜äº304ã€‚åº”ç”¨ï¼šé£Ÿå“å·¥ä¸šã€æµ·æ´‹è®¾å¤‡ã€‚",
    },
    {
        "name": "2205åŒç›¸é’¢",
        "content": "ç‰Œå·ï¼š2205 (S31803)ã€‚åŒ–å­¦æˆåˆ†ï¼šé“¬22%ï¼Œé•5%ï¼Œé’¼3%ï¼Œæ°®0.17%ã€‚ç‰¹æ€§ï¼šåŒç›¸ç»“æ„ï¼Œå¼ºåº¦æ˜¯316Lçš„ä¸¤å€ï¼Œæå¥½çš„æŠ—åº”åŠ›è…èš€å¼€è£‚ã€‚åº”ç”¨ï¼šé«˜æ°¯ç¯å¢ƒã€æµ·æ°´æ·¡åŒ–ã€‚",
    },
    {
        "name": "TA2çº¯é’› (Gr2)",
        "content": "ç‰Œå·ï¼šTA2 (ASTM Gr2)ã€‚åŒ–å­¦æˆåˆ†ï¼šå·¥ä¸šçº¯é’›ã€‚ç‰¹æ€§ï¼šä¼˜å¼‚çš„è€æµ·æ°´è…èš€æ€§ï¼Œå¯†åº¦ä½(4.51)ï¼Œå¡‘æ€§å¥½ï¼Œæ˜“ç„Šæ¥ã€‚åº”ç”¨ï¼šæµ·æ°´æ¢çƒ­å™¨ã€ç”µé•€è®¾å¤‡ã€‚å¼ºåº¦é€‚ä¸­ã€‚",
    },
    {
        "name": "TC4é’›åˆé‡‘ (Gr5)",
        "content": "ç‰Œå·ï¼šTC4 (ASTM Gr5)ã€‚åŒ–å­¦æˆåˆ†ï¼šTi-6Al-4Vã€‚ç‰¹æ€§ï¼šå¼ºåº¦æé«˜ï¼Œä½†å¡‘æ€§å·®ï¼Œéš¾å˜å½¢ï¼Œç„Šæ¥éœ€ä¿æŠ¤ã€‚åº”ç”¨ï¼šèˆªç©ºèˆªå¤©ã€é«˜å¼ºåº¦ç»“æ„ä»¶ã€‚ä¸å»ºè®®ç”¨äºè¿™å°±æ¢çƒ­æ‰©ç®¡ã€‚",
    }
]

# æŠŠæ•°æ®å˜æˆå¯¹è±¡
docs = [Document(page_content=d["content"], metadata={"name": d["name"]}) for d in steel_data]

# === B. åˆå§‹åŒ–æ¨¡å‹ (è‡ªåŠ¨è¯»å–ç¯å¢ƒå˜é‡) ===
# æ³¨æ„ï¼šDeepSeek å…¼å®¹ OpenAI åè®®ï¼Œæ‰€ä»¥è¿™é‡Œä¾ç„¶ç”¨ ChatOpenAI ç±»
llm = ChatOpenAI(
    model="deepseek-chat", # è¿™é‡Œå¡«æ¨¡å‹åå­—ï¼Œå¦‚æœæ˜¯ DeepSeek V3 å°±å¡« deepseek-chat
    temperature=0
)

embeddings = OpenAIEmbeddings(
    # DeepSeek ç›®å‰æ²¡æœ‰Embeddingæ¨¡å‹ï¼Œè¿™é‡Œæˆ‘ä»¬é€šå¸¸ç”¨ OpenAI çš„ text-embedding-3-small
    # æˆ–è€…ç”¨æœ¬åœ°çš„ HuggingFaceEmbeddings (ä¸ºäº†ç®€å•ï¼Œå…ˆå‡è®¾ä½ æœ‰ OpenAI key ç”¨æ¥åš embeddingï¼Œæˆ–è€… DeepSeek å°†æ¥æ”¯æŒ)
    # *æ³¨ï¼šä¸ºäº†è®©ä½ è·‘é€šï¼Œå¦‚æœä½ åªæœ‰ DeepSeek Keyï¼Œè¿™é‡Œå¯èƒ½ä¼šæŠ¥é”™ã€‚
    # ä¸´æ—¶è§£å†³æ–¹æ¡ˆï¼šDeepSeek ç”¨æˆ·é€šå¸¸æ­é…ä¸€ä¸ªå…è´¹çš„ embedding æ¨¡å‹ï¼Œæˆ–è€…åªéœ€å°‘é‡ OpenAI é¢åº¦*
)

# === C. å‘é‡åŒ–å­˜å…¥æœ¬åœ°æ•°æ®åº“ ===
# persist_directory æ˜¯æ•°æ®åº“å­˜ç¡¬ç›˜çš„æ–‡ä»¶å¤¹
vector_db = Chroma.from_documents(
    documents=docs, 
    embedding=embeddings,
    persist_directory="./chroma_db"
)
print("âœ… çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼")

# === D. å®šä¹‰ä¸“å®¶é€»è¾‘ ===
PROMPT_TEMPLATE = """
ä½ æ˜¯ä¸€ä½ç‰¹ç§é‡‘å±ææ–™ä¸“å®¶ã€‚åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡(Context)å›ç­”é—®é¢˜ã€‚
å¦‚æœä¸Šä¸‹æ–‡é‡Œæ²¡æœ‰ç­”æ¡ˆï¼Œå°±è¯´â€œèµ„æ–™åº“é‡Œæ²¡æŸ¥åˆ°â€ã€‚

ã€èµ„æ–™åº“æ•°æ®ã€‘ï¼š
{context}

ã€å®¢æˆ·é—®é¢˜ã€‘ï¼š
{question}
"""

def ask_expert(question):
    # 1. æ£€ç´¢ï¼šå»æ•°æ®åº“æ‰¾æœ€ç›¸å…³çš„2æ¡
    results = vector_db.similarity_search(question, k=2)
    
    # 2. æ‹¼å‡‘ä¸Šä¸‹æ–‡
    context_text = "\n\n".join([doc.page_content for doc in results])
    
    # 3. æé—®
    prompt = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)
    chain = prompt | llm
    
    response = chain.invoke({"context": context_text, "question": question})
    return response.content

# === E. è¿è¡Œæµ‹è¯• ===
if __name__ == "__main__":
    print("\nğŸ’¬ æ­£åœ¨å’¨è¯¢ AI ä¸“å®¶...\n")
    
    q1 = "æˆ‘è¦åšæµ·æ°´æ¢çƒ­å™¨ï¼Œç”¨TC4è¿˜æ˜¯TA2ï¼Ÿ"
    print(f"é—®ï¼š{q1}")
    print(f"ç­”ï¼š{ask_expert(q1)}")
    
    print("-" * 30)
    
    q2 = "316Lé‡Œå«æœ‰ä»€ä¹ˆæˆåˆ†ï¼Ÿ"
    print(f"é—®ï¼š{q2}")
    print(f"ç­”ï¼š{ask_expert(q2)}")
